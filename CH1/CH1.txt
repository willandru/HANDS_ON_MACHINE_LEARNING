UNSUPERVISED LEARNING:Unlabeled data
	-Clustering algorithm (hierarchical clustering): To detect grups and subgrups whitin the data.
	-Visualization algorithms: Outputs a 2D and 3D representation of the data.
	-Dimensionality reduction: Merging correlated features into one. Feature extraction. Trying to reduce the dimensions of your training data before feeding into another machine learning algorithm
	-Anomaly detection: Shown mostly normal instances during training.
	-Novelty detection: Detects new instances never seen in the training data (u need very clean data).
	-Association Rule Learning: Dig into large amounts of data and discover relations (supermarket shopping example).

SEMISUPERVISED LEARNING: Some algorithms can deal with data that is partially labeled.
	- Google's photo person detection. Unsupervised part: Automatically recognizes every person on the photos; Supervised part: You have to tell the name of the person. Now it's able to name everyone en every photo.
REINFORCMENT LEARINING: The agent can observe the environment, select and perform actions, and get rewards in return( or penalties), it must learn by itself whats the best strategy, called "policy" to get the most reward over time.
	-Policy: Defines what action the agent should choose when it is in a given situation.

BATCH LEARNING: The model must be trained using all the available data. The system is trained, then its launched to production, it just applies what is has learned (offline learning).

ONLINE LEARNING: You train the system incrementally, feeding data sequentially, either individually or in small groups (mini-batches)
- Good when the systems need to adapt rapidly or autonomously.
- Can be used to train systems in huge datasets, when they do not fit in ones machine (out-of-core learning done offline)
- Learning rate: How rapidly adapt to new data(rapidly adapting to new data also tends to forget the old data raidly).
-If a sensor gets broken or the feeding data is poor, the system will rapidly decline . You will need to check your system periodically and make sure you can revert to previus states. Also you may monitor your input data and react to abnormal data.

INSTANCE-BASED LEARNING: A type of model taht generalizes examples it has never seen before, or make predictions.
- You need a measure of similarity between two instances. 
-It generalizes on an instances based on the similarityes this instance has whit its closets instances,

MODEL-BASED LEARNING: Another way to generalize from a set of examples is to build a model of these examples, and then make predictions.
	

	LINEAR REGRESSION PROBLEM: 
	Equation of a simple linear model: You decide to model "life satisfaction" as a linear function of GDP per capita. This step is called "model selection":

					life_satisfaction = Theta_0 + (Theta_1* GDP_per_capita)

		How to choose the best parameters?: You need to specify a performances measure:

			-Utility function (fitness function): How good your model is.
			-Cost function : How bad it is.
		"Tipically , for Linear Regression Problems, use a cost function that measures the distance between the linear model's prediction and the training examples; the objective is to minimize the distance."


TRAINING A MODEL: Means runing an algorithm to find the model parameters that will make it best fitthe training data (and hopfully make good predictions)

	Replacing the Linear Regression Model with k-Nearest Neighbors regression is as simple as:

	import sklearn.linear_model
	model= sklearn.linear_model.LinearRegression()

	with:

	import sklearn.neighbors
	model = sklearn.neighbors.KNeighborsRegressor(n_neighbors=3)

NON-REPRESENTATIVE DATA

	-sampling noise: if the sample data is too small
	-samplin bias: If teh sampling method is flawed
		-nonresponse bias
	POOR-QUALITY DATA
	IRRELEVANT FEATURES : Feature engineering:
		-Feature selection (selecting the most useful)
		- Feature extraction (combining existing features to produce a more useful one)
		-Creating new features by gathering new data.

BAD ALGORITHMS
	
	OVERFITTING TRAINING DATA: The model performs well in the training data but it dows not generalzes well
	-Regularization: COnstraining a model to make it simpler and reduce the risk of overfitting
	For ezample in the above example if we force Theta_1=0 , the algorithm will have only one degree of freedom
	-Hyperparameter : Controls the amount of regularization to apply during leaning.
	An hyperparameter is a parameter of a learning algorithm (not of the model). It is not affected by the algorithm, it is set prior training and kept constant. TUrnung hyperparameters is an important part of building a MAchine Learning system

UNDERFITTING THE TRAINING DATA: 

	It occurs when the model is to simple to learn the underlying structure of the data.