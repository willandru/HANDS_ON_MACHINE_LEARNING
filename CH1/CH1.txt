UNSUPERVISED LEARNING:Unlabeled data
	-Clustering algorithm (hierarchical clustering): To detect grups and subgrups whitin the data.
	-Visualization algorithms: Outputs a 2D and 3D representation of the data.
	-Dimensionality reduction: Merging correlated features into one. Feature extraction. Trying to reduce the dimensions of your training data before feeding into another machine learning algorithm
	-Anomaly detection: Shown mostly normal instances during training.
	-Novelty detection: Detects new instances never seen in the training data (u need very clean data).
	-Association Rule Learning: Dig into large amounts of data and discover relations (supermarket shopping example).

SEMISUPERVISED LEARNING: Some algorithms can deal with data that is partially labeled.
	- Google's photo person detection. Unsupervised part: Automatically recognizes every person on the photos; Supervised part: You have to tell the name of the person. Now it's able to name everyone en every photo.
REINFORCMENT LEARINING: The agent can observe the environment, select and perform actions, and get rewards in return( or penalties), it must learn by itself whats the best strategy, called "policy" to get the most reward over time.
	-Policy: Defines what action the agent should choose when it is in a given situation.

BATCH LEARNING: The model must be trained using all the available data. The system is trained, then its launched to production, it just applies what is has learned (offline learning).

ONLINE LEARNING: You train the system incrementally, feeding data sequentially, either individually or in small groups (mini-batches)
- Good when the systems need to adapt rapidly or autonomously.
- Can be used to train systems in huge datasets, when they do not fit in ones machine (out-of-core learning done offline)
- Learning rate: How rapidly adapt to new data(rapidly adapting to new data also tends to forget the old data raidly).
-If a sensor gets broken or the feeding data is poor, the system will rapidly decline . You will need to check your system periodically and make sure you can revert to previus states. Also you may monitor your input data and react to abnormal data.

INSTANCE-BASED LEARNING: A type of model taht generalizes examples it has never seen before, or make predictions.
- You need a measure of similarity between two instances. 
-It generalizes on an instances based on the similarityes this instance has whit its closets instances,

MODEL-BASED LEARNING: Another way to generalize from a set of examples is to build a model of these examples, and then make predictions.
	

	LINEAR REGRESSION PROBLEM: 
	Equation of a simple linear model: You decide to model "life satisfaction" as a linear function of GDP per capita. This step is called "model selection":

					life_satisfaction = Theta_0 + (Theta_1* GDP_per_capita)

		How to choose the best parameters?: You need to specify a performances measure:

			-Utility function (fitness function): How good your model is.
			-Cost function : How bad it is.
		"Tipically , for Linear Regression Problems, use a cost function that measures the distance between the linear model's prediction and the training examples; the objective is to minimize the distance."


TRAINING A MODEL: Means runing an algorithm to find the model parameters that will make it best fitthe training data (and hopfully make good predictions)

	Replacing the Linear Regression Model with k-Nearest Neighbors regression is as simple as:

	import sklearn.linear_model
	model= sklearn.linear_model.LinearRegression()

	with:

	import sklearn.neighbors
	model = sklearn.neighbors.KNeighborsRegressor(n_neighbors=3)

NON-REPRESENTATIVE DATA

	-sampling noise: if the sample data is too small
	-samplin bias: If teh sampling method is flawed
		-nonresponse bias
	POOR-QUALITY DATA
	IRRELEVANT FEATURES : Feature engineering:
		-Feature selection (selecting the most useful)
		- Feature extraction (combining existing features to produce a more useful one)
		-Creating new features by gathering new data.

BAD ALGORITHMS
	
	OVERFITTING TRAINING DATA: The model performs well in the training data but it dows not generalzes well
	-Regularization: COnstraining a model to make it simpler and reduce the risk of overfitting
	For ezample in the above example if we force Theta_1=0 , the algorithm will have only one degree of freedom
	-Hyperparameter : Controls the amount of regularization to apply during leaning.
	An hyperparameter is a parameter of a learning algorithm (not of the model). It is not affected by the algorithm, it is set prior training and kept constant. TUrnung hyperparameters is an important part of building a MAchine Learning system

UNDERFITTING THE TRAINING DATA: 

	It occurs when the model is to simple to learn the underlying structure of the data.

TESTING AND VALIDATING

	Split the data into training set and test set

HYPERPARAMETER TUNING AND MODEL SELECTION

	¿How to choose the value of the regularization hyperparameter? (Train 100 different models usign 100 different hyperparameters (?))

	HOLD OUT VALIDATION: You hold out part of the training set to evaluate several candidate models and select the best one.
						The new held out set is called the VALIDATION SET (or DEVELOPMENT SET or DEV SET).
						More specifically you train multiple models whit varius hypermparameters on the reduced training set, and you select
						the model that performs the best in the validation set.
						After this hold out validation process, you train the best model on the best training set

						-If the validation set is too small you will end up whit a bad model

						-If the validation set is too large, the remaining training set will be much smaller than the full training set.
							It is not ideal to compare candidate models trained on a much smaller training set.
							One way to solve this is to perform repeated CROSS-VALIDATION, using many small validation sets.
							CROSS-VALIDATION
								Each model is evaluated once per validation set after it's trained on the rest of the data. 
								By averaging out all the evaluations of a model, you get a much more accurate measure of its performance.

								-The training time is multiplied by the number of validation sets.


			~10.000 representative instances in your DATA SET.{ it is a small data set}

			Andrew Ng ----->  Training Set | Train-Dev Set ---> Train the model on the training set and evaluate it on the train-dev set to see if its overfiting your actual data (taraining set).



			NO FREE LUNCH THEOREM

			To decide what data to discard and what data to keep, you must make assumptions

			"If you make absolutely no assumptions about the data, then there is no reason to prefer one model over any other." <-- NO FREE LUCNH THEOREM (NFL).

			*There is no model that is a priori guaranteed to work better

	¿How would you define Machine Learning? : Its the hability of a computer to numerically observe the data and crate correlations and observations on that data using different algorithms, thatn enables the machines to understand the data and perform different actions whits its found knoledge.
	¿Can you name four types of problems where it shines? It can be used to visually represent data sets tht have hundreds of dimensions into 2D and 3D plots. It can be used to clasiffy instances of the data or real world. 